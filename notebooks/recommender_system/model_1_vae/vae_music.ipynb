{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tracks_orig = pd.read_json(\"../../../data/v2/tracks.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess tracks data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from src.track_preprocessor.VAEPreprocessor import VAEPreprocessor\n",
    "from src.recommender_playlist_provider.common.CallType import CallType"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "vae_preprocessor = VAEPreprocessor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                       id                                               name  \\\n0  0RNxWy0PC3AyH4ThH3aGK6                                     Mack the Knife   \n1  2W889aLIKxULEefrleFBFI                           Someone to Watch Over Me   \n2  4Pnzw1nLOpDNV6MKI5ueIR     Nancy (With the Laughing Face) - 78rpm Version   \n3  7GLmfKOe5BfOXk7334DoKt  Saturday Night (Is The Loneliest Night In The ...   \n4  6kD1SNGPkfX9LwaGd1FG92             Put Your Dreams Away (For Another Day)   \n\n   popularity  duration_ms  explicit               id_artist release_date  \\\n0          55       201467         0  19eLuQmk9aCobbVDHc6eek         1929   \n1          54       198000         0  1Mxqyy3pSjf8kZZL4QVxS0         1943   \n2          55       199000         0  1Mxqyy3pSjf8kZZL4QVxS0         1944   \n3          54       163000         0  1Mxqyy3pSjf8kZZL4QVxS0         1944   \n4          53       186173         0  1Mxqyy3pSjf8kZZL4QVxS0         1944   \n\n   danceability  energy  key  loudness  speechiness  acousticness  \\\n0         0.673  0.3770    0   -14.141       0.0697         0.586   \n1         0.204  0.1510    2   -17.842       0.0418         0.947   \n2         0.295  0.0826    1   -19.569       0.0367         0.984   \n3         0.561  0.3350    9   -11.093       0.0499         0.840   \n4         0.197  0.0546    1   -22.411       0.0346         0.950   \n\n   instrumentalness  liveness  valence    tempo  \n0          0.000000     0.332    0.713   88.973  \n1          0.000009     0.321    0.134   91.783  \n2          0.000358     0.156    0.169  128.600  \n3          0.000002     0.788    0.590  126.974  \n4          0.276000     0.152    0.100   90.150  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>popularity</th>\n      <th>duration_ms</th>\n      <th>explicit</th>\n      <th>id_artist</th>\n      <th>release_date</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>key</th>\n      <th>loudness</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0RNxWy0PC3AyH4ThH3aGK6</td>\n      <td>Mack the Knife</td>\n      <td>55</td>\n      <td>201467</td>\n      <td>0</td>\n      <td>19eLuQmk9aCobbVDHc6eek</td>\n      <td>1929</td>\n      <td>0.673</td>\n      <td>0.3770</td>\n      <td>0</td>\n      <td>-14.141</td>\n      <td>0.0697</td>\n      <td>0.586</td>\n      <td>0.000000</td>\n      <td>0.332</td>\n      <td>0.713</td>\n      <td>88.973</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2W889aLIKxULEefrleFBFI</td>\n      <td>Someone to Watch Over Me</td>\n      <td>54</td>\n      <td>198000</td>\n      <td>0</td>\n      <td>1Mxqyy3pSjf8kZZL4QVxS0</td>\n      <td>1943</td>\n      <td>0.204</td>\n      <td>0.1510</td>\n      <td>2</td>\n      <td>-17.842</td>\n      <td>0.0418</td>\n      <td>0.947</td>\n      <td>0.000009</td>\n      <td>0.321</td>\n      <td>0.134</td>\n      <td>91.783</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4Pnzw1nLOpDNV6MKI5ueIR</td>\n      <td>Nancy (With the Laughing Face) - 78rpm Version</td>\n      <td>55</td>\n      <td>199000</td>\n      <td>0</td>\n      <td>1Mxqyy3pSjf8kZZL4QVxS0</td>\n      <td>1944</td>\n      <td>0.295</td>\n      <td>0.0826</td>\n      <td>1</td>\n      <td>-19.569</td>\n      <td>0.0367</td>\n      <td>0.984</td>\n      <td>0.000358</td>\n      <td>0.156</td>\n      <td>0.169</td>\n      <td>128.600</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7GLmfKOe5BfOXk7334DoKt</td>\n      <td>Saturday Night (Is The Loneliest Night In The ...</td>\n      <td>54</td>\n      <td>163000</td>\n      <td>0</td>\n      <td>1Mxqyy3pSjf8kZZL4QVxS0</td>\n      <td>1944</td>\n      <td>0.561</td>\n      <td>0.3350</td>\n      <td>9</td>\n      <td>-11.093</td>\n      <td>0.0499</td>\n      <td>0.840</td>\n      <td>0.000002</td>\n      <td>0.788</td>\n      <td>0.590</td>\n      <td>126.974</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6kD1SNGPkfX9LwaGd1FG92</td>\n      <td>Put Your Dreams Away (For Another Day)</td>\n      <td>53</td>\n      <td>186173</td>\n      <td>0</td>\n      <td>1Mxqyy3pSjf8kZZL4QVxS0</td>\n      <td>1944</td>\n      <td>0.197</td>\n      <td>0.0546</td>\n      <td>1</td>\n      <td>-22.411</td>\n      <td>0.0346</td>\n      <td>0.950</td>\n      <td>0.276000</td>\n      <td>0.152</td>\n      <td>0.100</td>\n      <td>90.150</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_orig.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0        0RNxWy0PC3AyH4ThH3aGK6\n1        2W889aLIKxULEefrleFBFI\n2        4Pnzw1nLOpDNV6MKI5ueIR\n3        7GLmfKOe5BfOXk7334DoKt\n4        6kD1SNGPkfX9LwaGd1FG92\n                  ...          \n22407    0LcNMuOiULmxJK3bdHTfDF\n22408    1uviKYHZuM4uINK33F7sCt\n22409    1fXmDeiCb3ABt5CzkMxp4u\n22410    27kcZEJvhkb1rzZS9gCpdA\n22411    27Y1N4Q4U3EfDU5Ubw8ws2\nName: id, Length: 22412, dtype: object"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_ids = tracks_orig.id\n",
    "tracks_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "       duration_ms  popularity  explicit  release_date  danceability  \\\n0        -0.397693   -0.792191 -0.447118     -5.542188      0.466992   \n1        -0.445783   -0.916444 -0.447118     -4.536483     -2.496578   \n2        -0.431913   -0.792191 -0.447118     -4.464647     -1.921557   \n3        -0.931261   -0.916444 -0.447118     -4.464647     -0.240726   \n4        -0.609833   -1.040697 -0.447118     -4.464647     -2.540810   \n...            ...         ...       ...           ...           ...   \n22407    -0.210160    0.450344 -0.447118      0.923058     -0.392380   \n22408    -0.434701    1.071611 -0.447118      0.994894     -0.670412   \n22409    -0.475356    0.201837 -0.447118      0.994894     -0.133305   \n22410    -0.385376    0.698851 -0.447118      0.994894     -0.057478   \n22411    -0.590026    1.071611 -0.447118      0.994894     -0.405018   \n\n         energy  loudness  speechiness  acousticness  instrumentalness  ...  \\\n0     -1.293191 -1.857878    -0.147948      1.152614         -0.263179  ...   \n1     -2.371576 -2.847975    -0.460325      2.460088         -0.263119  ...   \n2     -2.697954 -3.309985    -0.517426      2.594095         -0.260837  ...   \n3     -1.493599 -1.042471    -0.369635      2.072554         -0.263169  ...   \n4     -2.831560 -4.070282    -0.540938      2.470953          1.542815  ...   \n...         ...       ...          ...           ...               ...  ...   \n22407 -0.004855 -0.046488    -0.347242     -0.361304         -0.263179  ...   \n22408 -0.892375 -0.355208    -0.417779      2.090663         -0.263179  ...   \n22409  0.438905  0.566137    -0.121076     -0.926669         -0.263179  ...   \n22410 -0.978264  0.410439    -0.119957      1.268512         -0.263179  ...   \n22411 -1.593803 -1.505284    -0.471521      2.271754         -0.262198  ...   \n\n       2  3  4  5  6  7  8  9  10  11  \n0      0  0  0  0  0  0  0  0   0   0  \n1      1  0  0  0  0  0  0  0   0   0  \n2      0  0  0  0  0  0  0  0   0   0  \n3      0  0  0  0  0  0  0  1   0   0  \n4      0  0  0  0  0  0  0  0   0   0  \n...   .. .. .. .. .. .. .. ..  ..  ..  \n22407  1  0  0  0  0  0  0  0   0   0  \n22408  1  0  0  0  0  0  0  0   0   0  \n22409  1  0  0  0  0  0  0  0   0   0  \n22410  0  0  0  0  0  0  0  0   0   0  \n22411  0  0  0  0  0  1  0  0   0   0  \n\n[22412 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>duration_ms</th>\n      <th>popularity</th>\n      <th>explicit</th>\n      <th>release_date</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>loudness</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>...</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.397693</td>\n      <td>-0.792191</td>\n      <td>-0.447118</td>\n      <td>-5.542188</td>\n      <td>0.466992</td>\n      <td>-1.293191</td>\n      <td>-1.857878</td>\n      <td>-0.147948</td>\n      <td>1.152614</td>\n      <td>-0.263179</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.445783</td>\n      <td>-0.916444</td>\n      <td>-0.447118</td>\n      <td>-4.536483</td>\n      <td>-2.496578</td>\n      <td>-2.371576</td>\n      <td>-2.847975</td>\n      <td>-0.460325</td>\n      <td>2.460088</td>\n      <td>-0.263119</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.431913</td>\n      <td>-0.792191</td>\n      <td>-0.447118</td>\n      <td>-4.464647</td>\n      <td>-1.921557</td>\n      <td>-2.697954</td>\n      <td>-3.309985</td>\n      <td>-0.517426</td>\n      <td>2.594095</td>\n      <td>-0.260837</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.931261</td>\n      <td>-0.916444</td>\n      <td>-0.447118</td>\n      <td>-4.464647</td>\n      <td>-0.240726</td>\n      <td>-1.493599</td>\n      <td>-1.042471</td>\n      <td>-0.369635</td>\n      <td>2.072554</td>\n      <td>-0.263169</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.609833</td>\n      <td>-1.040697</td>\n      <td>-0.447118</td>\n      <td>-4.464647</td>\n      <td>-2.540810</td>\n      <td>-2.831560</td>\n      <td>-4.070282</td>\n      <td>-0.540938</td>\n      <td>2.470953</td>\n      <td>1.542815</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22407</th>\n      <td>-0.210160</td>\n      <td>0.450344</td>\n      <td>-0.447118</td>\n      <td>0.923058</td>\n      <td>-0.392380</td>\n      <td>-0.004855</td>\n      <td>-0.046488</td>\n      <td>-0.347242</td>\n      <td>-0.361304</td>\n      <td>-0.263179</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22408</th>\n      <td>-0.434701</td>\n      <td>1.071611</td>\n      <td>-0.447118</td>\n      <td>0.994894</td>\n      <td>-0.670412</td>\n      <td>-0.892375</td>\n      <td>-0.355208</td>\n      <td>-0.417779</td>\n      <td>2.090663</td>\n      <td>-0.263179</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22409</th>\n      <td>-0.475356</td>\n      <td>0.201837</td>\n      <td>-0.447118</td>\n      <td>0.994894</td>\n      <td>-0.133305</td>\n      <td>0.438905</td>\n      <td>0.566137</td>\n      <td>-0.121076</td>\n      <td>-0.926669</td>\n      <td>-0.263179</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22410</th>\n      <td>-0.385376</td>\n      <td>0.698851</td>\n      <td>-0.447118</td>\n      <td>0.994894</td>\n      <td>-0.057478</td>\n      <td>-0.978264</td>\n      <td>0.410439</td>\n      <td>-0.119957</td>\n      <td>1.268512</td>\n      <td>-0.263179</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22411</th>\n      <td>-0.590026</td>\n      <td>1.071611</td>\n      <td>-0.447118</td>\n      <td>0.994894</td>\n      <td>-0.405018</td>\n      <td>-1.593803</td>\n      <td>-1.505284</td>\n      <td>-0.471521</td>\n      <td>2.271754</td>\n      <td>-0.262198</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>22412 rows × 25 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_tracks = vae_preprocessor.preprocess_tracks(tracks_data=tracks_orig, call_type=CallType.TRAINING)\n",
    "preprocessed_tracks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train VAE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, decomposition, manifold, preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import time\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.compat.v1.keras.layers import Input, Dense, LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D,  Concatenate\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import Dense, Reshape, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, logcosh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_features (InputLayer)    [(None, 25)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           1664        ['input_features[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64)          256         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          8320        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128)         512         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8256        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64)          256         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           2080        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32)          128         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 32)           0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 128)          4224        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 128)          4224        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 128)          0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,920\n",
      "Trainable params: 29,344\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z (InputLayer)              [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 25)                1625      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,593\n",
      "Trainable params: 25,017\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "############################ Define Models ############################\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "input_dim = 25\n",
    "latent_dim = 128\n",
    "\n",
    "############################ encoder ############################\n",
    "input_features = keras.Input(shape=(input_dim, ), name=\"input_features\")\n",
    "\n",
    "x = Dense(64, activation=\"relu\")(input_features)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = keras.Model(inputs=input_features, outputs=[z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "############################ decoder ############################\n",
    "latent_inputs = Input(shape=(latent_dim,), name=\"z\")\n",
    "x = Dense(32, activation=\"relu\")(latent_inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(input_dim, activation=\"linear\")(x)\n",
    "\n",
    "# Define the VAE model\n",
    "decoder = keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Print the summary of the models\n",
    "decoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def kl_loss(z_mean, z_log_var):\n",
    "    \"\"\"Calculates the KL divergence loss.\"\"\"\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "    return kl_loss\n",
    "\n",
    "def reconstruction_loss(inputs, reconstructed):\n",
    "    mse = tf.keras.losses.MeanAbsoluteError()\n",
    "    loss = mse(inputs, reconstructed)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z_mean, z_log_var, z = encoder(inputs_batch)\n",
    "        reconstruction = decoder(z)\n",
    "        reconstruction_losses = reconstruction_loss(inputs_batch, reconstruction)\n",
    "\n",
    "        # kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        # kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        total_loss = reconstruction_losses\n",
    "\n",
    "    grads = tape.gradient(total_loss, decoder.trainable_weights+encoder.trainable_weights)\n",
    "    vae_optimizer.apply_gradients(zip(grads, decoder.trainable_weights+encoder.trainable_weights))\n",
    "    return total_loss\n",
    "\n",
    "def train(train_dataset, epochs):\n",
    "    step=0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for input_batch in train_dataset:\n",
    "            loss = train_step(input_batch)\n",
    "            epoch_loss += loss\n",
    "            step += 1\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {epoch_loss / len(train_dataset):.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(preprocessed_tracks.values).batch(256)\n",
    "train_dataset = train_dataset.shuffle(1024)\n",
    "\n",
    "# train_dataset = train_dataset.map(lambda x: (tf.reshape(x, [-1, 25]), x))\n",
    "vae_optimizer = tf.keras.optimizers.Adam()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.4111\n",
      "Epoch 2: Loss = 0.3685\n",
      "Epoch 3: Loss = 0.3386\n",
      "Epoch 4: Loss = 0.3272\n",
      "Epoch 5: Loss = 0.3128\n",
      "Epoch 6: Loss = 0.3022\n",
      "Epoch 7: Loss = 0.2970\n",
      "Epoch 8: Loss = 0.2974\n",
      "Epoch 9: Loss = 0.2948\n",
      "Epoch 10: Loss = 0.2904\n",
      "Epoch 11: Loss = 0.2878\n",
      "Epoch 12: Loss = 0.2865\n",
      "Epoch 13: Loss = 0.2850\n",
      "Epoch 14: Loss = 0.2812\n",
      "Epoch 15: Loss = 0.2822\n",
      "Epoch 16: Loss = 0.2791\n",
      "Epoch 17: Loss = 0.2762\n",
      "Epoch 18: Loss = 0.2761\n",
      "Epoch 19: Loss = 0.2752\n",
      "Epoch 20: Loss = 0.2724\n",
      "Epoch 21: Loss = 0.2754\n",
      "Epoch 22: Loss = 0.2772\n",
      "Epoch 23: Loss = 0.2725\n",
      "Epoch 24: Loss = 0.2685\n",
      "Epoch 25: Loss = 0.2659\n",
      "Epoch 26: Loss = 0.2679\n",
      "Epoch 27: Loss = 0.2564\n",
      "Epoch 28: Loss = 0.2553\n",
      "Epoch 29: Loss = 0.2550\n",
      "Epoch 30: Loss = 0.2536\n",
      "Epoch 31: Loss = 0.2504\n",
      "Epoch 32: Loss = 0.2495\n",
      "Epoch 33: Loss = 0.2554\n",
      "Epoch 34: Loss = 0.2496\n",
      "Epoch 35: Loss = 0.2463\n",
      "Epoch 36: Loss = 0.2452\n",
      "Epoch 37: Loss = 0.2477\n",
      "Epoch 38: Loss = 0.2475\n",
      "Epoch 39: Loss = 0.2426\n",
      "Epoch 40: Loss = 0.2442\n",
      "Epoch 41: Loss = 0.2445\n",
      "Epoch 42: Loss = 0.2476\n",
      "Epoch 43: Loss = 0.2432\n",
      "Epoch 44: Loss = 0.2436\n",
      "Epoch 45: Loss = 0.2402\n",
      "Epoch 46: Loss = 0.2425\n",
      "Epoch 47: Loss = 0.2393\n",
      "Epoch 48: Loss = 0.2363\n",
      "Epoch 49: Loss = 0.2379\n",
      "Epoch 50: Loss = 0.2346\n",
      "Epoch 51: Loss = 0.2366\n",
      "Epoch 52: Loss = 0.2309\n",
      "Epoch 53: Loss = 0.2276\n",
      "Epoch 54: Loss = 0.2246\n",
      "Epoch 55: Loss = 0.2274\n",
      "Epoch 56: Loss = 0.2193\n",
      "Epoch 57: Loss = 0.2237\n",
      "Epoch 58: Loss = 0.2172\n",
      "Epoch 59: Loss = 0.2235\n",
      "Epoch 60: Loss = 0.2174\n",
      "Epoch 61: Loss = 0.2146\n",
      "Epoch 62: Loss = 0.2138\n",
      "Epoch 63: Loss = 0.2140\n",
      "Epoch 64: Loss = 0.2105\n",
      "Epoch 65: Loss = 0.2090\n",
      "Epoch 66: Loss = 0.2043\n",
      "Epoch 67: Loss = 0.2024\n",
      "Epoch 68: Loss = 0.1979\n",
      "Epoch 69: Loss = 0.1986\n",
      "Epoch 70: Loss = 0.1966\n",
      "Epoch 71: Loss = 0.1952\n",
      "Epoch 72: Loss = 0.1947\n",
      "Epoch 73: Loss = 0.1966\n",
      "Epoch 74: Loss = 0.2007\n",
      "Epoch 75: Loss = 0.1934\n",
      "Epoch 76: Loss = 0.1908\n",
      "Epoch 77: Loss = 0.1912\n",
      "Epoch 78: Loss = 0.1925\n",
      "Epoch 79: Loss = 0.1897\n",
      "Epoch 80: Loss = 0.1937\n",
      "Epoch 81: Loss = 0.1938\n",
      "Epoch 82: Loss = 0.1892\n",
      "Epoch 83: Loss = 0.1898\n",
      "Epoch 84: Loss = 0.1927\n",
      "Epoch 85: Loss = 0.1936\n",
      "Epoch 86: Loss = 0.1862\n",
      "Epoch 87: Loss = 0.1851\n",
      "Epoch 88: Loss = 0.1856\n",
      "Epoch 89: Loss = 0.1817\n",
      "Epoch 90: Loss = 0.1833\n",
      "Epoch 91: Loss = 0.1787\n",
      "Epoch 92: Loss = 0.1819\n",
      "Epoch 93: Loss = 0.1799\n",
      "Epoch 94: Loss = 0.1782\n",
      "Epoch 95: Loss = 0.1843\n",
      "Epoch 96: Loss = 0.1782\n",
      "Epoch 97: Loss = 0.1836\n",
      "Epoch 98: Loss = 0.1794\n",
      "Epoch 99: Loss = 0.1768\n",
      "Epoch 100: Loss = 0.1777\n",
      "Epoch 101: Loss = 0.1768\n",
      "Epoch 102: Loss = 0.1803\n",
      "Epoch 103: Loss = 0.1774\n",
      "Epoch 104: Loss = 0.1747\n",
      "Epoch 105: Loss = 0.1765\n",
      "Epoch 106: Loss = 0.1766\n",
      "Epoch 107: Loss = 0.1757\n",
      "Epoch 108: Loss = 0.1777\n",
      "Epoch 109: Loss = 0.1751\n",
      "Epoch 110: Loss = 0.1759\n",
      "Epoch 111: Loss = 0.1729\n",
      "Epoch 112: Loss = 0.1740\n",
      "Epoch 113: Loss = 0.1756\n",
      "Epoch 114: Loss = 0.1733\n",
      "Epoch 115: Loss = 0.1727\n",
      "Epoch 116: Loss = 0.1723\n",
      "Epoch 117: Loss = 0.1722\n",
      "Epoch 118: Loss = 0.1734\n",
      "Epoch 119: Loss = 0.1769\n",
      "Epoch 120: Loss = 0.1732\n",
      "Epoch 121: Loss = 0.1756\n",
      "Epoch 122: Loss = 0.1717\n",
      "Epoch 123: Loss = 0.1704\n",
      "Epoch 124: Loss = 0.1702\n",
      "Epoch 125: Loss = 0.1709\n",
      "Epoch 126: Loss = 0.1751\n",
      "Epoch 127: Loss = 0.1723\n",
      "Epoch 128: Loss = 0.1708\n",
      "Epoch 129: Loss = 0.1723\n",
      "Epoch 130: Loss = 0.1697\n",
      "Epoch 131: Loss = 0.1702\n",
      "Epoch 132: Loss = 0.1688\n",
      "Epoch 133: Loss = 0.1742\n",
      "Epoch 134: Loss = 0.1748\n",
      "Epoch 135: Loss = 0.1694\n",
      "Epoch 136: Loss = 0.1703\n",
      "Epoch 137: Loss = 0.1692\n",
      "Epoch 138: Loss = 0.1692\n",
      "Epoch 139: Loss = 0.1714\n",
      "Epoch 140: Loss = 0.1697\n",
      "Epoch 141: Loss = 0.1698\n",
      "Epoch 142: Loss = 0.1713\n",
      "Epoch 143: Loss = 0.1707\n",
      "Epoch 144: Loss = 0.1690\n",
      "Epoch 145: Loss = 0.1675\n",
      "Epoch 146: Loss = 0.1694\n",
      "Epoch 147: Loss = 0.1676\n",
      "Epoch 148: Loss = 0.1681\n",
      "Epoch 149: Loss = 0.1669\n",
      "Epoch 150: Loss = 0.1665\n",
      "Epoch 151: Loss = 0.1697\n",
      "Epoch 152: Loss = 0.1666\n",
      "Epoch 153: Loss = 0.1671\n",
      "Epoch 154: Loss = 0.1685\n",
      "Epoch 155: Loss = 0.1648\n",
      "Epoch 156: Loss = 0.1689\n",
      "Epoch 157: Loss = 0.1676\n",
      "Epoch 158: Loss = 0.1656\n",
      "Epoch 159: Loss = 0.1661\n",
      "Epoch 160: Loss = 0.1655\n",
      "Epoch 161: Loss = 0.1642\n",
      "Epoch 162: Loss = 0.1679\n",
      "Epoch 163: Loss = 0.1652\n",
      "Epoch 164: Loss = 0.1671\n",
      "Epoch 165: Loss = 0.1648\n",
      "Epoch 166: Loss = 0.1660\n",
      "Epoch 167: Loss = 0.1642\n",
      "Epoch 168: Loss = 0.1670\n",
      "Epoch 169: Loss = 0.1625\n",
      "Epoch 170: Loss = 0.1629\n",
      "Epoch 171: Loss = 0.1646\n",
      "Epoch 172: Loss = 0.1643\n",
      "Epoch 173: Loss = 0.1637\n",
      "Epoch 174: Loss = 0.1631\n",
      "Epoch 175: Loss = 0.1645\n",
      "Epoch 176: Loss = 0.1637\n",
      "Epoch 177: Loss = 0.1607\n",
      "Epoch 178: Loss = 0.1585\n",
      "Epoch 179: Loss = 0.1594\n",
      "Epoch 180: Loss = 0.1572\n",
      "Epoch 181: Loss = 0.1574\n",
      "Epoch 182: Loss = 0.1546\n",
      "Epoch 183: Loss = 0.1584\n",
      "Epoch 184: Loss = 0.1557\n",
      "Epoch 185: Loss = 0.1565\n",
      "Epoch 186: Loss = 0.1541\n",
      "Epoch 187: Loss = 0.1546\n",
      "Epoch 188: Loss = 0.1509\n",
      "Epoch 189: Loss = 0.1533\n",
      "Epoch 190: Loss = 0.1527\n",
      "Epoch 191: Loss = 0.1524\n",
      "Epoch 192: Loss = 0.1514\n",
      "Epoch 193: Loss = 0.1530\n",
      "Epoch 194: Loss = 0.1541\n",
      "Epoch 195: Loss = 0.1522\n",
      "Epoch 196: Loss = 0.1516\n",
      "Epoch 197: Loss = 0.1506\n",
      "Epoch 198: Loss = 0.1518\n",
      "Epoch 199: Loss = 0.1485\n",
      "Epoch 200: Loss = 0.1484\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, epochs=200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ONLY reconstruction loss\n",
    "* Batch norm and dropout\n",
    "    * 64 latent space | 200 epochs | 0.1654 loss\n",
    "    * 128 latent space | 200 epochs | 0.1511 loss\n",
    "* Without batch norm and dropout\n",
    "    * 64 latent space | 200 epochs | 0.0797 loss   | no intersection between recommended and listened for 40 recommendations\n",
    "    * 128 latent space | 200 epochs | 0.1397 loss  |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "MODELS_PATHS = \"../../models/\"\n",
    "encoder.compile()\n",
    "encoder.save((os.path.join(MODELS_PATHS, \"encoder_3.h5\")))\n",
    "decoder.compile()\n",
    "decoder.save((os.path.join(MODELS_PATHS, \"decoder_3.h5\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Produce embeddings of all tracks in a latent space and save them"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22412, 25)\n",
      "701/701 [==============================] - 3s 4ms/step\n",
      "(22412, 128)\n"
     ]
    }
   ],
   "source": [
    "all_tracks = preprocessed_tracks.values\n",
    "print(all_tracks.shape)\n",
    "_, _, tracks_latent_space = encoder.predict(all_tracks)\n",
    "# tracks in latent space\n",
    "print(tracks_latent_space.shape)\n",
    "# Save the ndarray to a file\n",
    "np.save('../../models/embeddings_of_all_tracks_3.npy', tracks_latent_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save ids of all tracks in the same order as embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "np.save('../../models/track_ids_3.npy', tracks_ids.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "MODELS_PATHS = \"../../models/\"\n",
    "MODEL = (os.path.join(MODELS_PATHS, \"encoder.h5\"))\n",
    "encoder = tf.keras.models.load_model(MODEL, compile=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation and tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get some track from user 108"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['7D8aQaRzoi9Qzz5yerVK5b',\n '2DAsLftcRKP3iarCPmI1RY',\n '29YDZUNKvBI2I64TvLj1Wt',\n '1LitgN2tFmNVEtLY6qh5SZ',\n '2gCvWjrHt6PVJjIN1amlje',\n '6bLy8YiVKRpUzwWZG6lVDq',\n '4Yf5bqU3NK4kNOypcrLYwU',\n '06EgH8BdWW9QRrlI6ZVQhW',\n '3Tows9RnoAq9CmMJaII2cO',\n '0mHyWYXmmCB9iQyK18m3FQ',\n '5eALoO9RRACvoxvLKF9zt4',\n '67v0HTNTkJmbhxOipBunXe',\n '0B0tYbVp7pDQAqKDhgMeaL',\n '4YDdWcdzHhLDZ5d1j9nRaJ',\n '5BK5IvPzmTZBGsCnYRjSue',\n '5N877LRsRxO4lHHHWHwoOv',\n '7safX55XidhznxK5eDdDm5',\n '0b7ZmVPawkWSRxQlaLTo3J',\n '6vw2D0AeWBSJhJi0OrnudX',\n '2QbcikdPIWOLA9Uumiis1X',\n '7zAt4tdL44D3VuzsvM0N8n',\n '3AR1c3Dssq51WlGGkuYJNj',\n '1jM23jASF0JrhgqYhqCZfl',\n '15FcP9qwmIKqaD5NhfhNpu',\n '7kcG2nk2Y9O1VEIyfjQO9p',\n '60l2m3BD5VY0HSc3xmSpPI',\n '4scS449npSaFolwj9b4RJD',\n '0yMoJXVP6hFLV71DRVxRTk',\n '2rKcC5vlTsDM94RWTPoyHV',\n '3o9JsjPISLo2T9rcXLDT0a',\n '33rh7azo556EGptA220qxT',\n '6NF2cF3kRFGqhAuqj17yzD',\n '58WOkUl3O9LvLZjdhiQvIX',\n '2lKA9bNdd4kAoeHiufa0aK',\n '0OtgQMnnZL5KVjpABf6SdR',\n '53jogSv23P6DFcOHcZrDs9',\n '1T8f6NtVBK3f4jcKgCjnvw',\n '44ADyYoY5liaRa3EOAl4uf',\n '1MrZ8hGkUWMmT816wPaMgE',\n '6wcWYONsa1wGcmhu3WX7SK',\n '0Rcm3eyf9ALtx69QUldYIf',\n '2O2ii9OPZYh1NBXo9FtE0Y',\n '78JmElAFmrPNhLjovDR9Jm',\n '1TWfkGrhF7ob0nwB2M6knb',\n '3BqqF8suAIzW8655yJfcvh',\n '4rylUVBblCUdmtDuYql6oI',\n '5Jno63iv0mzu5OZ28asYbE',\n '095MMFhB9qxPx2VsmvjnUs',\n '0NWPxcsf5vdjdiFUI8NgkP',\n '4rhQ89gUsCW5ig91qtoDSE',\n '6vsyag9kEPckt19NClSf51',\n '0KMGxYKeUzK9wc5DZCt3HT',\n '4ZkhFcoS3apzze9w2yI9NO',\n '2CvOqDpQIMw69cCzWqr5yr',\n '3cOO5IQtOYs7huq4Z6lYfr',\n '1iN1knAnE9tgY3FipGocKX',\n '26dOCdbA4tZIJh9pGOLuHx',\n '6vPAmoERUMRoTZaCCSWQ12',\n '6Q6YqGj1Ku1CUGHWSFwSHY',\n '6rROXfjFzQyH2w9FAiVkq7',\n '1d5UuboIPRMD4HaU3yycKC',\n '6Awg3vdlBZnLnVWBcyG0vW',\n '5U4wYRHrCRxRP6iQfM824C']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.user_sessions_fetcher.common.UserSessionsProvider import UserSessionsProvider\n",
    "\n",
    "from src.user_sessions_fetcher.common.constants import (EVENT_TYPE_PLAY, EVENT_TYPE_LIKE)\n",
    "usp = UserSessionsProvider(\"../../../data/test\")\n",
    "session_track_ids_user_108_like = usp.get_user_sessions(108, [EVENT_TYPE_LIKE])\n",
    "session_track_ids_user_108_like"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test VAEPlaylistProvider"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from src.recommender_playlist_provider.vae.VAEPlaylistProvider import VAEPlaylistProvider"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "vae_pp = VAEPlaylistProvider(model_path=\"../../models/encoder_3.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "vae_pp.load_model_from_file()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "                           id                          name  popularity  \\\n107    78JmElAFmrPNhLjovDR9Jm  All Day and All of the Night          69   \n255    0NWPxcsf5vdjdiFUI8NgkP                       Hey Joe          72   \n450    4ZkhFcoS3apzze9w2yI9NO     Thank You - 1990 Remaster          57   \n613    4rylUVBblCUdmtDuYql6oI   Don't Let It Bring You Down          51   \n637    3BqqF8suAIzW8655yJfcvh            Morning Has Broken          70   \n...                       ...                           ...         ...   \n17733  7D8aQaRzoi9Qzz5yerVK5b                   Throw A Fit          65   \n18266  58WOkUl3O9LvLZjdhiQvIX                  El Mismo Sol          57   \n18649  67v0HTNTkJmbhxOipBunXe           Para Que Seas Feliz          52   \n19767  6Awg3vdlBZnLnVWBcyG0vW               Runaway (U & I)          58   \n21191  33rh7azo556EGptA220qxT                      Physical          60   \n\n       duration_ms  explicit               id_artist release_date  \\\n107         141507         0  1SQRv42e4PjEYfPhS0Tk9E   1964-10-02   \n255         210160         0  776Uo845nYHJpNaStv1Ds4   1967-05-12   \n450         289467         0  36QJpDe2go2KgaRleHCDTp   1969-10-22   \n613         176867         0  6v8FB84lnmJs434UJf2Mrm   1970-08-31   \n637         200000         0  08F3Y3SctIlsOEmKd6dnH8   1971-01-01   \n...            ...       ...                     ...          ...   \n17733       158757         1  0NIIxcxNHmOoyBx03SfTCD   2018-07-27   \n18266       179507         0  2urF8dgLVfDjunO0pcHUEe   2016-07-15   \n18649       289907         0  3tJnB0s6c3oXPq1SCCavnd   1996-01-01   \n19767       227074         0  4sTQVOfp9vEMCemLw50sbu   2015-06-05   \n21191       193829         0  6M2wZ9GZgrQXHCFfjv46we   2020-03-27   \n\n       danceability  energy  key  loudness  speechiness  acousticness  \\\n107           0.551   0.860    7    -7.839       0.0796       0.26200   \n255           0.346   0.768    9    -5.695       0.0377       0.00603   \n450           0.316   0.270    7   -14.429       0.0590       0.17100   \n613           0.662   0.237    5   -11.851       0.0369       0.50200   \n637           0.424   0.321    7   -13.162       0.0312       0.62200   \n...             ...     ...  ...       ...          ...           ...   \n17733         0.888   0.586    0    -6.745       0.0604       0.00597   \n18266         0.732   0.820    8    -5.948       0.0427       0.32300   \n18649         0.689   0.309    5   -11.465       0.0290       0.49700   \n19767         0.506   0.805    1    -4.119       0.0469       0.00711   \n21191         0.643   0.813    0    -4.819       0.0492       0.01680   \n\n       instrumentalness  liveness  valence    tempo  \n107            0.000005    0.0558   0.7230  136.921  \n255            0.380000    0.0244   0.5320  169.492  \n450            0.002250    0.1650   0.0956   78.476  \n613            0.000003    0.1070   0.2800  133.074  \n637            0.019600    0.0903   0.4050  133.126  \n...                 ...       ...      ...      ...  \n17733          0.036500    0.1530   0.4010  150.051  \n18266          0.003490    0.0868   0.9780  112.991  \n18649          0.000027    0.1050   0.3460   89.977  \n19767          0.001930    0.0856   0.3830  126.008  \n21191          0.000344    0.1030   0.7470  146.983  \n\n[63 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>popularity</th>\n      <th>duration_ms</th>\n      <th>explicit</th>\n      <th>id_artist</th>\n      <th>release_date</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>key</th>\n      <th>loudness</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>107</th>\n      <td>78JmElAFmrPNhLjovDR9Jm</td>\n      <td>All Day and All of the Night</td>\n      <td>69</td>\n      <td>141507</td>\n      <td>0</td>\n      <td>1SQRv42e4PjEYfPhS0Tk9E</td>\n      <td>1964-10-02</td>\n      <td>0.551</td>\n      <td>0.860</td>\n      <td>7</td>\n      <td>-7.839</td>\n      <td>0.0796</td>\n      <td>0.26200</td>\n      <td>0.000005</td>\n      <td>0.0558</td>\n      <td>0.7230</td>\n      <td>136.921</td>\n    </tr>\n    <tr>\n      <th>255</th>\n      <td>0NWPxcsf5vdjdiFUI8NgkP</td>\n      <td>Hey Joe</td>\n      <td>72</td>\n      <td>210160</td>\n      <td>0</td>\n      <td>776Uo845nYHJpNaStv1Ds4</td>\n      <td>1967-05-12</td>\n      <td>0.346</td>\n      <td>0.768</td>\n      <td>9</td>\n      <td>-5.695</td>\n      <td>0.0377</td>\n      <td>0.00603</td>\n      <td>0.380000</td>\n      <td>0.0244</td>\n      <td>0.5320</td>\n      <td>169.492</td>\n    </tr>\n    <tr>\n      <th>450</th>\n      <td>4ZkhFcoS3apzze9w2yI9NO</td>\n      <td>Thank You - 1990 Remaster</td>\n      <td>57</td>\n      <td>289467</td>\n      <td>0</td>\n      <td>36QJpDe2go2KgaRleHCDTp</td>\n      <td>1969-10-22</td>\n      <td>0.316</td>\n      <td>0.270</td>\n      <td>7</td>\n      <td>-14.429</td>\n      <td>0.0590</td>\n      <td>0.17100</td>\n      <td>0.002250</td>\n      <td>0.1650</td>\n      <td>0.0956</td>\n      <td>78.476</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>4rylUVBblCUdmtDuYql6oI</td>\n      <td>Don't Let It Bring You Down</td>\n      <td>51</td>\n      <td>176867</td>\n      <td>0</td>\n      <td>6v8FB84lnmJs434UJf2Mrm</td>\n      <td>1970-08-31</td>\n      <td>0.662</td>\n      <td>0.237</td>\n      <td>5</td>\n      <td>-11.851</td>\n      <td>0.0369</td>\n      <td>0.50200</td>\n      <td>0.000003</td>\n      <td>0.1070</td>\n      <td>0.2800</td>\n      <td>133.074</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>3BqqF8suAIzW8655yJfcvh</td>\n      <td>Morning Has Broken</td>\n      <td>70</td>\n      <td>200000</td>\n      <td>0</td>\n      <td>08F3Y3SctIlsOEmKd6dnH8</td>\n      <td>1971-01-01</td>\n      <td>0.424</td>\n      <td>0.321</td>\n      <td>7</td>\n      <td>-13.162</td>\n      <td>0.0312</td>\n      <td>0.62200</td>\n      <td>0.019600</td>\n      <td>0.0903</td>\n      <td>0.4050</td>\n      <td>133.126</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17733</th>\n      <td>7D8aQaRzoi9Qzz5yerVK5b</td>\n      <td>Throw A Fit</td>\n      <td>65</td>\n      <td>158757</td>\n      <td>1</td>\n      <td>0NIIxcxNHmOoyBx03SfTCD</td>\n      <td>2018-07-27</td>\n      <td>0.888</td>\n      <td>0.586</td>\n      <td>0</td>\n      <td>-6.745</td>\n      <td>0.0604</td>\n      <td>0.00597</td>\n      <td>0.036500</td>\n      <td>0.1530</td>\n      <td>0.4010</td>\n      <td>150.051</td>\n    </tr>\n    <tr>\n      <th>18266</th>\n      <td>58WOkUl3O9LvLZjdhiQvIX</td>\n      <td>El Mismo Sol</td>\n      <td>57</td>\n      <td>179507</td>\n      <td>0</td>\n      <td>2urF8dgLVfDjunO0pcHUEe</td>\n      <td>2016-07-15</td>\n      <td>0.732</td>\n      <td>0.820</td>\n      <td>8</td>\n      <td>-5.948</td>\n      <td>0.0427</td>\n      <td>0.32300</td>\n      <td>0.003490</td>\n      <td>0.0868</td>\n      <td>0.9780</td>\n      <td>112.991</td>\n    </tr>\n    <tr>\n      <th>18649</th>\n      <td>67v0HTNTkJmbhxOipBunXe</td>\n      <td>Para Que Seas Feliz</td>\n      <td>52</td>\n      <td>289907</td>\n      <td>0</td>\n      <td>3tJnB0s6c3oXPq1SCCavnd</td>\n      <td>1996-01-01</td>\n      <td>0.689</td>\n      <td>0.309</td>\n      <td>5</td>\n      <td>-11.465</td>\n      <td>0.0290</td>\n      <td>0.49700</td>\n      <td>0.000027</td>\n      <td>0.1050</td>\n      <td>0.3460</td>\n      <td>89.977</td>\n    </tr>\n    <tr>\n      <th>19767</th>\n      <td>6Awg3vdlBZnLnVWBcyG0vW</td>\n      <td>Runaway (U &amp; I)</td>\n      <td>58</td>\n      <td>227074</td>\n      <td>0</td>\n      <td>4sTQVOfp9vEMCemLw50sbu</td>\n      <td>2015-06-05</td>\n      <td>0.506</td>\n      <td>0.805</td>\n      <td>1</td>\n      <td>-4.119</td>\n      <td>0.0469</td>\n      <td>0.00711</td>\n      <td>0.001930</td>\n      <td>0.0856</td>\n      <td>0.3830</td>\n      <td>126.008</td>\n    </tr>\n    <tr>\n      <th>21191</th>\n      <td>33rh7azo556EGptA220qxT</td>\n      <td>Physical</td>\n      <td>60</td>\n      <td>193829</td>\n      <td>0</td>\n      <td>6M2wZ9GZgrQXHCFfjv46we</td>\n      <td>2020-03-27</td>\n      <td>0.643</td>\n      <td>0.813</td>\n      <td>0</td>\n      <td>-4.819</td>\n      <td>0.0492</td>\n      <td>0.01680</td>\n      <td>0.000344</td>\n      <td>0.1030</td>\n      <td>0.7470</td>\n      <td>146.983</td>\n    </tr>\n  </tbody>\n</table>\n<p>63 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tracks = vae_pp.get_features_of_track_ids(session_track_ids_user_108_like)\n",
    "features_tracks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PB\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "track_recommendations = vae_pp.predict_recommendations(session_track_ids_user_108_like, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['2rKcC5vlTsDM94RWTPoyHV', '1iN1knAnE9tgY3FipGocKX',\n       '4scS449npSaFolwj9b4RJD', '5eALoO9RRACvoxvLKF9zt4',\n       '0NWPxcsf5vdjdiFUI8NgkP', '4rylUVBblCUdmtDuYql6oI',\n       '3BqqF8suAIzW8655yJfcvh', '6vw2D0AeWBSJhJi0OrnudX',\n       '26dOCdbA4tZIJh9pGOLuHx', '78JmElAFmrPNhLjovDR9Jm'], dtype=object)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_recommendations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "['7AM7gEehnVqElO1BD9h8dC',\n '1tQy2AqgFc0AdddmIboIfX',\n '1GmqbthsQEpnLiU1hnlFVn',\n '7D8aQaRzoi9Qzz5yerVK5b',\n '7D8aQaRzoi9Qzz5yerVK5b',\n '4Musyaro0NM5Awx8b5c627',\n '4hrAKlxfblnG7llBEEX7TR',\n '2DAsLftcRKP3iarCPmI1RY',\n '2DAsLftcRKP3iarCPmI1RY',\n '3jnoftwNCmIuTNVkxakisg',\n '29YDZUNKvBI2I64TvLj1Wt',\n '29YDZUNKvBI2I64TvLj1Wt',\n '3ljr9ATeLs2BY9gNp7vm62',\n '1LitgN2tFmNVEtLY6qh5SZ',\n '1LitgN2tFmNVEtLY6qh5SZ',\n '2gCvWjrHt6PVJjIN1amlje',\n '2gCvWjrHt6PVJjIN1amlje',\n '6bLy8YiVKRpUzwWZG6lVDq',\n '6bLy8YiVKRpUzwWZG6lVDq',\n '4Yf5bqU3NK4kNOypcrLYwU',\n '4Yf5bqU3NK4kNOypcrLYwU',\n '0MrZG1nTNQVzeSJKi6gFVF',\n '49GpGYGli1xcxovgYf0k4c',\n '4DRFIoachXrW8YV7Yje0yz',\n '5nHTLEJ10zaqdnKqLriah4',\n '06EgH8BdWW9QRrlI6ZVQhW',\n '06EgH8BdWW9QRrlI6ZVQhW',\n '7DiaNCPgH5dJ1cJ925jTAA',\n '3Tows9RnoAq9CmMJaII2cO',\n '3Tows9RnoAq9CmMJaII2cO',\n '0mHyWYXmmCB9iQyK18m3FQ',\n '0mHyWYXmmCB9iQyK18m3FQ',\n '5VjlmjJ1peXIjcuecCdfx1',\n '2su4VuRfiwInV18U5apPdi',\n '5eALoO9RRACvoxvLKF9zt4',\n '5eALoO9RRACvoxvLKF9zt4',\n '3kZUnG3jVpgwh3TQFdTv18',\n '6gXrEUzibufX9xYPk3HD5p',\n '67v0HTNTkJmbhxOipBunXe',\n '67v0HTNTkJmbhxOipBunXe',\n '0B0tYbVp7pDQAqKDhgMeaL',\n '0B0tYbVp7pDQAqKDhgMeaL',\n '2CIDqYfIYfl9vx02wIyCG9',\n '7HrzErXq3TsKOY1gmdIShB',\n '6dmueYtoihUhlQZlImBCmm',\n '1zIshc2P9l5AZsOpJWmdTM',\n '4YDdWcdzHhLDZ5d1j9nRaJ',\n '4YDdWcdzHhLDZ5d1j9nRaJ',\n '5JCZHWdFLg2rXtg684rTHt',\n '5BK5IvPzmTZBGsCnYRjSue',\n '5BK5IvPzmTZBGsCnYRjSue',\n '02TVrXmI9EBYpuSdmTr4gn',\n '2dlEdDEmuQsrcXaAL3Znzi',\n '0VQ0n5gMgJLNixG8BfWBT5',\n '2inyRCfWB1ey6vNX43TTCl',\n '5N877LRsRxO4lHHHWHwoOv',\n '5N877LRsRxO4lHHHWHwoOv',\n '5A32KQZznC2HSqr9qzTl2N',\n '2QWP8NYYplOqEFBYGCcq0S',\n '7safX55XidhznxK5eDdDm5',\n '7safX55XidhznxK5eDdDm5',\n '0b7ZmVPawkWSRxQlaLTo3J',\n '0b7ZmVPawkWSRxQlaLTo3J',\n '6vw2D0AeWBSJhJi0OrnudX',\n '6vw2D0AeWBSJhJi0OrnudX',\n '5BpmwiKR6ns1EtfCQRpNUc',\n '7DHApOHsIFo7EyfieArYGE',\n '6Dtq63g5m2sMIVd1Aw9ZDD',\n '3FrX3mx8qq7SZX2NYbzDoj',\n '7qhDd43Mw3H2dRvh6i6CYT',\n '2QbcikdPIWOLA9Uumiis1X',\n '2QbcikdPIWOLA9Uumiis1X',\n '7zAt4tdL44D3VuzsvM0N8n',\n '7zAt4tdL44D3VuzsvM0N8n',\n '3xIyTWewE9LFZwy3HklGwE',\n '0UzsDmdpw0Q14KU4hieQss',\n '31acMiV67UgKn1ScFChFxo',\n '4nwUQZbmWMkYzKrXgWRIEw',\n '3AR1c3Dssq51WlGGkuYJNj',\n '3AR1c3Dssq51WlGGkuYJNj',\n '1jM23jASF0JrhgqYhqCZfl',\n '1jM23jASF0JrhgqYhqCZfl',\n '15FcP9qwmIKqaD5NhfhNpu',\n '15FcP9qwmIKqaD5NhfhNpu',\n '469RmLzEsicj8VSn9aIuKV',\n '4kOTDj4SiZPkW1p96WMDNv',\n '3d3k8g4GTVx9EFIFlOZOEX',\n '2IQHkgihsAvSxuCh7uwikt',\n '5fkDD7aQLUmMy1EwzNm0ZB',\n '5J0aNEUTxJWKXnQmyY3vUp',\n '7kcG2nk2Y9O1VEIyfjQO9p',\n '7kcG2nk2Y9O1VEIyfjQO9p',\n '4b93D55xv3YCH5mT4p6HPn',\n '4Tf94tAHtlQxjbeeyU3y7C',\n '60l2m3BD5VY0HSc3xmSpPI',\n '60l2m3BD5VY0HSc3xmSpPI',\n '4scS449npSaFolwj9b4RJD',\n '4scS449npSaFolwj9b4RJD',\n '7JJmb5XwzOO8jgpou264Ml',\n '0yMoJXVP6hFLV71DRVxRTk',\n '0yMoJXVP6hFLV71DRVxRTk',\n '376Gg7Oxo5C5jUWDWpVXeI',\n '3mmi9qWiD7wt9yIq8fmw1Q',\n '2BstRQGodshjGpeDGQiNgo',\n '4IablJ6SqVNGY4vrseyKxu',\n '1hNE737dkYD9ZM1J8H9chv',\n '7LzouaWGFCy4tkXDOOnEyM',\n '5Qevo5QToL2p7q1umdLtTC',\n '2rKcC5vlTsDM94RWTPoyHV',\n '2rKcC5vlTsDM94RWTPoyHV',\n '7agqowjjStyVeTdJPM2yMl',\n '3o9JsjPISLo2T9rcXLDT0a',\n '3o9JsjPISLo2T9rcXLDT0a',\n '5fUMiojNarx83jSyA1MhFH',\n '33rh7azo556EGptA220qxT',\n '33rh7azo556EGptA220qxT',\n '5Dg2h1wsm7ZijCo0yLmbvR',\n '6NF2cF3kRFGqhAuqj17yzD',\n '6NF2cF3kRFGqhAuqj17yzD',\n '0fqP1OK8UuiVK4WdsYCeY3',\n '3ybF6Iaia01EbXg5VmxPcb',\n '58WOkUl3O9LvLZjdhiQvIX',\n '58WOkUl3O9LvLZjdhiQvIX',\n '6J0AjzrYsPZrQtw6IjErhy',\n '7FRfYOql61DGDp9VPPe2qA',\n '3Ie2eLOIj2IhKnzPwXrLbJ',\n '75CgD6l7K4qMzZrn4CbZqz',\n '6Gyk7ZHfFWo3d8U7poUEPs',\n '3OYZWMm5m2DEwq2Tc1ukTh',\n '2lKA9bNdd4kAoeHiufa0aK',\n '2lKA9bNdd4kAoeHiufa0aK',\n '7hHTyS1QG0TcX5iQ0sa1Tk',\n '0nrRP2bk19rLc0orkWPQk2',\n '0e06Zwd1027jFtmrRbXAjl',\n '254bXAqt3zP6P50BdQvEsq',\n '5tdWyz9U3pbmenWJfG1bDj',\n '0OtgQMnnZL5KVjpABf6SdR',\n '0OtgQMnnZL5KVjpABf6SdR',\n '235LXPXfi0SmOaS9TaCh3c',\n '18a9EGG4xhRELv7bgAw5hb',\n '53jogSv23P6DFcOHcZrDs9',\n '53jogSv23P6DFcOHcZrDs9',\n '0dkvbYmXYEPK7gGMdytmfI',\n '5OBptQtIlGOG94la7qil6c',\n '1c3mgJu23QqajyOV7isUXc',\n '0wzRcekWyVCSyPtlPOeJau',\n '1urmwhtXPiakhcqvqUi3rp',\n '3P4v70V3Zt804r2c9dZivK',\n '1T8f6NtVBK3f4jcKgCjnvw',\n '1T8f6NtVBK3f4jcKgCjnvw',\n '1HOlb9rdNOmy9b1Fakicjo',\n '17S4XrLvF5jlGvGCJHgF51',\n '44ADyYoY5liaRa3EOAl4uf',\n '44ADyYoY5liaRa3EOAl4uf',\n '1MrZ8hGkUWMmT816wPaMgE',\n '1MrZ8hGkUWMmT816wPaMgE',\n '0KAqMRUSZwzG3dZLdDA4eH',\n '28H3wOEKsaEa8NBEYCLl4j',\n '0YWUHZPJVg4iujddsJDwhM',\n '5gWtkdgdyt5bZt9i6n3Kqd',\n '5TGHpNdHeV5cqsJfkOO4Zd',\n '05zCWOpwIUf6JY06gSL6uz',\n '1XFSSbQ9fApWNXZMAKllb6',\n '6wcWYONsa1wGcmhu3WX7SK',\n '6wcWYONsa1wGcmhu3WX7SK',\n '0Rcm3eyf9ALtx69QUldYIf',\n '0Rcm3eyf9ALtx69QUldYIf',\n '5iua35KoPKyet49uHr7aaU',\n '4PMx9RSiKlgry322H27rrZ',\n '1Q5kgpp4pmyGqPwNBzkSrw',\n '2O2ii9OPZYh1NBXo9FtE0Y',\n '2O2ii9OPZYh1NBXo9FtE0Y',\n '78JmElAFmrPNhLjovDR9Jm',\n '78JmElAFmrPNhLjovDR9Jm',\n '7lX5thFFq1mEARXVPgJ1op',\n '1TWfkGrhF7ob0nwB2M6knb',\n '1TWfkGrhF7ob0nwB2M6knb',\n '15iosIuxC3C53BgsM5Uggs',\n '2zLLGT8WWT1dl73TlVL9qs',\n '1f6XyjXx6wUb7zdk9JWzl9',\n '5JF4qhrnmULBxThWkLzUq5',\n '6aaPUBUFw9KEW1p1inVQv9',\n '7iKSx0WjzCgYYaM7o1R5CV',\n '3BqqF8suAIzW8655yJfcvh',\n '3BqqF8suAIzW8655yJfcvh',\n '4rylUVBblCUdmtDuYql6oI',\n '4rylUVBblCUdmtDuYql6oI',\n '2JuasWPUodaUxf5nwNpciQ',\n '5Jno63iv0mzu5OZ28asYbE',\n '5Jno63iv0mzu5OZ28asYbE',\n '5qxfr8OdO79j2bSvHPUCL5',\n '095MMFhB9qxPx2VsmvjnUs',\n '095MMFhB9qxPx2VsmvjnUs',\n '0LkssuXRYPFYapjXH9pJBX',\n '0NWPxcsf5vdjdiFUI8NgkP',\n '0NWPxcsf5vdjdiFUI8NgkP',\n '5vr2aeKPuH8koWDcjRvlEn',\n '4rhQ89gUsCW5ig91qtoDSE',\n '4rhQ89gUsCW5ig91qtoDSE',\n '7CFQrZR4WeKEg4vweqp8Gv',\n '6vsyag9kEPckt19NClSf51',\n '6vsyag9kEPckt19NClSf51',\n '2pmtavvgqZN52EePYxutgk',\n '0KMGxYKeUzK9wc5DZCt3HT',\n '0KMGxYKeUzK9wc5DZCt3HT',\n '4ZkhFcoS3apzze9w2yI9NO',\n '4ZkhFcoS3apzze9w2yI9NO',\n '30brAfx64jxQMeUNUgDFaj',\n '7Ar4G7Ci11gpt6sfH9Cgz5',\n '2CvOqDpQIMw69cCzWqr5yr',\n '2CvOqDpQIMw69cCzWqr5yr',\n '2fvLQe9ZlRaEJ5o4sG3TbM',\n '0DJnqFhVWoTDs58JPem5Zh',\n '3cOO5IQtOYs7huq4Z6lYfr',\n '3cOO5IQtOYs7huq4Z6lYfr',\n '7ynvnjTj1XI8ByUq3F8MUw',\n '1iN1knAnE9tgY3FipGocKX',\n '1iN1knAnE9tgY3FipGocKX',\n '02M6vucOvmRfMxTXDUwRXu',\n '26dOCdbA4tZIJh9pGOLuHx',\n '26dOCdbA4tZIJh9pGOLuHx',\n '6vPAmoERUMRoTZaCCSWQ12',\n '6vPAmoERUMRoTZaCCSWQ12',\n '6Q6YqGj1Ku1CUGHWSFwSHY',\n '6Q6YqGj1Ku1CUGHWSFwSHY',\n '4skuEIloXWuxxgekKupkEH',\n '1CnW361jtuyldTsP3pchjS',\n '6rROXfjFzQyH2w9FAiVkq7',\n '6rROXfjFzQyH2w9FAiVkq7',\n '1ykYncbYWgaHapHMJWyGqi',\n '4fqwEothoIOcKr3pARJCgu',\n '0FHNqVPxu14WivXR4Y2hFr',\n '6yV2W1h4PZla1PE5s1zjVM',\n '1d5UuboIPRMD4HaU3yycKC',\n '1d5UuboIPRMD4HaU3yycKC',\n '0W8NvcMsIuygeFnfNz2KnC',\n '6Awg3vdlBZnLnVWBcyG0vW',\n '6Awg3vdlBZnLnVWBcyG0vW',\n '4XdePl1vOF1cYjTBYeYU9j',\n '1qNjY6nEGbZ9XJ6rpaG0Eq',\n '3CCf4QWbFngXoIO1cLTj1L',\n '4rylUVBblCUdmtDuYql6oI',\n '5U4wYRHrCRxRP6iQfM824C',\n '5U4wYRHrCRxRP6iQfM824C']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_track_ids_user_108_non_liked = usp.get_user_sessions(108)  # get tracks' ids of user 108 from all sessions\n",
    "session_track_ids_user_108_non_liked"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "0    4J0DbyODwZJcmIAiTSJfMF\n1    01S5aiC3Q5bt84EiGaCVwP\n2    4J0DbyODwZJcmIAiTSJfMF\n3    1kN9UiyOSRVCrSc49ft0lp\n4    2pCwNrjm7IoAHAj2XqflCu\n5    0bYDebBlQxsDR4hCgbbpOW\n6    4J0DbyODwZJcmIAiTSJfMF\n7    2kgUPa3UUnHIUfySDkg2bm\n8    69foTA1ElY03DJQb7bbA0e\n9    3bjLA6ukBcGtvhe5tybxp7\ndtype: object"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(track_recommendations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "0    True\n1    True\n2    True\n3    True\n4    True\n5    True\n6    True\n7    True\n8    True\n9    True\ndtype: bool"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(track_recommendations).isin(pd.Series(session_track_ids_user_108_non_liked))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### All recommended tracks were listened by user"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}